# Preference based cost learning
This module implements a modular framework for learning cost parameters using human preference labels. The human queries are generated by jointly optimizing over the information gain over two trajectories.

## How to define a new learning problem (minimal cartpole example):
### Implement system dynamics:
```python
import torch
import numpy as np
from preference_learning.dynamics import Dynamics


# Implements a 4 state cartpole (x, theta, v, omega) that is controlled by the linear acceleration (a)
class CartPoleDynamics(Dynamics):
    x_idx = 0
    theta_idx = 1
    v_idx = 2
    omega_idx = 3

    a_idx = 0

    def __init__(self, dt: float, device: str):
        # Define control bounds, used for initializing the sampling distribution and penalizing constraint violations: 
        lbu = torch.tensor([-2]).to(device)
        ubu = torch.tensor([2]).to(device)
        # Define state bounds, used for clamping the states:
        lbx = torch.tensor([-10, np.deg2rad(-90), -2, np.deg2rad(-45)]).to(device)
        ubx = torch.tensor([10, np.deg2rad(90), 2, np.deg2rad(45)]).to(device)
        super().__init__(dt, lbu=lbu, ubu=ubu, lbx=lbx, ubx=ubx)

        # Define initial state sampling distribution:
        lbx_initial = torch.tensor([-2.5, np.deg2rad(-22.5), -1e-5, -1e-5]).to(device)
        ubx_initial = torch.tensor([2.5, np.deg2rad(22.5), 1e-5, 1e-5]).to(device)
        self.sampler = torch.distributions.uniform.Uniform(lbx_initial, ubx_initial)

    def _discrete_dynamics_fun(self, x_values: torch.Tensor, u_values: torch.Tensor, dt: float) -> torch.Tensor:
        g = 9.81        # Gravitational acceleration
        l = 1           # length of pole
        l_c = l / 2     # Pole's center of mass location
        m_p = 1         # Mass of pole
        m_c = 2         # Mass of cart

        theta = x_values[:, self.theta_idx]
        v = x_values[:, self.v_idx]
        theta_dot = x_values[:, self.omega_idx]
        a = u_values[:, self.a_idx]

        # Simplified dynamics calculation
        common_numerator = g * torch.sin(theta) - a * torch.cos(theta)
        common_denominator = l_c * (4/3 - m_p * torch.cos(theta).pow(2) / (m_p + m_c))
        theta_double_dot = common_numerator / common_denominator
        x_double_dot = a + m_p * l_c * theta_double_dot * torch.cos(theta) / (m_p + m_c) - m_p * l_c * theta_dot.pow(2) * torch.sin(theta) / (m_p + m_c)

        dot_state = torch.stack([
            v,  
            theta_dot, 
            x_double_dot, 
            theta_double_dot  
        ], dim=1)
        return x_values + dt * dot_state

    def generate_initial_state(self) -> torch.Tensor:
        x = self.sampler.sample()
        return x
```

### Define a parametrized cost function:
```python
import torch
from preference_learning.costs import Cost
from cartpole_preferences.dynamics import CartPoleDynamics
from typing import Dict

class CartPoleCost(Cost):
    def __init__(self, N: int, device: str):
        super().__init__()

        # Define trainable weights (to ensure positive semidefinite cost these are interpreted as logarithms):
        params = [
            'x_weight',
            'theta_weight',
            'v_weight',
            'omega_weight',
        ]
        for param_name in params:
            initialized_param = torch.nn.Parameter(torch.normal(mean=torch.tensor([0.]), std=torch.tensor([10.])).to(device))
            setattr(self, param_name, initialized_param)

    # Parametrized cost: C(x, params) = x Q x.T, with our parameters forming the diagonal of Q
    def _cost(self, x_values: torch.Tensor, p_values: torch.Tensor) -> torch.Tensor:
        x = x_values[:, :, CartPoleDynamics.x_idx]
        theta = x_values[:, :, CartPoleDynamics.theta_idx]
        v = x_values[:, :, CartPoleDynamics.v_idx]
        omega = x_values[:, :, CartPoleDynamics.omega_idx]
        C = (torch.exp(self.x_weight) * x**2) + (torch.exp(self.theta_weight) * theta**2) + (torch.exp(self.v_weight) * v**2) + (torch.exp(self.omega_weight) * omega**2)
        return C.sum(dim=1)

    def forward(self, x_values: torch.Tensor, p_values: torch.Tensor, return_terminal=False) -> torch.Tensor:
        stage_cost = self._cost(x_values[:, :-1, :], p_values)
        terminal_cost = torch.zeros(x_values.shape[0]).to(x_values.device)
        if not return_terminal:
            return stage_cost
        else:
            return stage_cost, terminal_cost
    
    # Return the parameter values, used for logging the mean/variance of our ensemble
    def get_parameters(self) -> Dict[str, float]:
        params = {k: torch.exp(v.item()) for k, v in self.state_dict().items()}
        return params
```

### Define a top level problem instance:
```python
```

### Start learning loop:
```python
import argparse
import torch
from preference_learning.dynamics import DualDynamics
from preference_learning.optimization import EvoTorchWrapper
from preference_learning.training_data import TrajectoryDataset
from cartpole_preferences.dynamics import CartPoleDynamics
from cartpole_preferences.costs import CartPoleCost
from cartpole_preferences.problem import CartPoleInfoGainProblem

if __name__ == "__main__":
    dynamics = CartPoleDynamics(dt=0.1, device="cuda:0") 
    cost = CartPoleCost
    dataset = TrajectoryDataset(directory="avant_data")
    problem = CartPoleInfoGainProblem(N=40, cost_module=cost, dynamics=dynamics, dataset=dataset, n_ensembles=10, parameters_path="avant_parameters", device="cuda:0")
    solver = EvoTorchWrapper(problem)
    solver.solve()
```
